{"id": "seed_task_0", "name": "Detect Multiple Origin AS (MOAS) Conflicts with PyBGPStream","instruction": "Generate a Python script using the PyBGPStream library to detect Multiple Origin AS (MOAS) conflicts in BGP data. The script should: Collect BGP updates for IPv4 prefixes during the time interval from '2020-01-01 00:00:00' to '2020-01-15 23:59:59' from the 'route-views' project. Identify MOAS conflicts where a prefix is announced by multiple origin ASes. Display the detected MOAS conflicts, indicating the prefix and the corresponding origin ASes.","instances": [    {        "input": "Python script using PyBGPStream to detect MOAS conflicts.",        "output": "import pybgpstream\nfrom collections import defaultdict\nfrom datetime import datetime\n\nmoas_dict = defaultdict(set)\n\nstream = pybgpstream.BGPStream(from_time=\"2020-01-01 00:00:00\", until_time=\"2020-01-15 23:59:59\", project=\"route-views\", record_type=\"updates\", filter=\"ipversion 4\")\n\nfor rec in stream.records():\n    timestamp = datetime.utcfromtimestamp(rec.time)\n    for elem in rec:\n        if elem.type == \"A\":\n            origin_as = elem.fields['as-path'].split(\" \")[-1]\n            prefix = elem.fields['prefix']\n            moas_dict[prefix].add(origin_as)\n\nprint(\"MOAS conflicts detected:\")\nfor prefix, origins in moas_dict.items():\n    if len(origins) > 1:\n        print(f\"Prefix: {prefix}, Origin ASes: {', '.join(origins)}\")"    }],"is_classification": false}
{"id": "seed_task_1", "name": "BGP Anomaly Detection in AS Path Lengths", "instruction": "Perform BGP analysis and detect anomalies in AS path lengths for IPv4 prefixes over two time periods.", "instances": [{"input": "BGP anomaly detection in AS path lengths for IPv4 prefixes over two time periods.", "output": "import pybgpstream\nfrom collections import defaultdict\nfrom datetime import datetime\nimport statistics\n\n# Data structure to hold average AS path lengths per prefix\naverage_path_lengths = defaultdict(list)\n\n# Initialize BGP Stream\nstream = pybgpstream.BGPStream(\n    from_time=\"2021-01-01 00:00:00\", until_time=\"2021-01-01 12:00:00\",\n    project=\"ris\",\n    collectors=\"rrc01,rrc02\",\n    record_type=\"updates\",\n    filter=\"ipversion 4\"\n)\n\n# Collect average path lengths for later comparison\nfor rec in stream.records():\n    for elem in rec:\n        if elem.type == \"A\":  # A for 'announcement'\n            as_path = elem.fields.get('as-path', '').split(\" \")\n            prefix = elem.fields.get('prefix', None)\n\n            if prefix and as_path:\n                path_length = len(as_path)\n                average_path_lengths[prefix].append(path_length)\n\n# Calculate average lengths per prefix\nfor prefix in average_path_lengths:\n    avg_length = statistics.mean(average_path_lengths[prefix])\n    average_path_lengths[prefix] = avg_length\n\n# Initialize a new stream for the period of interest\nstream = pybgpstream.BGPStream(\n    from_time=\"2021-01-02 00:00:00\", until_time=\"2021-01-02 12:00:00\",\n    project=\"ris\",\n    collectors=\"rrc01,rrc02\",\n    record_type=\"updates\",\n    filter=\"ipversion 4\"\n)\n\n# Detect anomalies in path lengths\nprint(\"Scanning for anomalies in AS path lengths...\")\nfor rec in stream.records():\n    for elem in rec:\n        if elem.type == \"A\":\n            as_path = elem.fields.get('as-path', '').split(\" \")\n            prefix = elem.fields.get('prefix', None)\n\n            if prefix and as_path and prefix in average_path_lengths:\n                path_length = len(as_path)\n                avg_length = average_path_lengths[prefix]\n\n                if path_length > avg_length * 1.5:  # Anomaly threshold\n                    print(f\"Anomaly detected for prefix {prefix}. Current length: {path_length}, Avg length: {avg_length}\")\n\n# The script can be extended to send alerts or store anomalous paths for further investigation."}], "is_classification": false}
{"id": "seed_task_2", "name": "Investigating BGP Community Local Preference Variations", "instruction": "Investigate variations in local preference values associated with BGP communities using data from collector rrc03 on July 15, 2021, between 10:00 and 11:00.", "instances": [{"input": "Investigating BGP community local preference variations using data from collector rrc03 on July 15, 2021, between 10:00 and 11:00.", "output": "import pybgpstream\nfrom collections import defaultdict\n\n# Initialize BGP Stream\nstream = pybgpstream.BGPStream(\n    from_time=\"2021-07-15 10:00:00\", until_time=\"2021-07-15 11:00:00\",\n    project=\"ris\",\n    collectors=\"rrc03\",\n    record_type=\"updates\",\n    filter=\"ipversion 4\"\n)\n\n# Map of communities to local preference values\ncommunity_to_local_pref = defaultdict(list)\n\n# Loop through the stream records\nfor rec in stream.records():\n    for elem in rec:\n        if elem.type == \"A\":\n            communities = elem.fields.get('communities', None)\n            local_pref = elem.fields.get('local_pref', None)\n\n            if communities and local_pref:\n                for community in communities.split(\",\"):\n                    community_to_local_pref[community].append(int(local_pref))\n\n# Calculate average local preference for each community\nprint(\"Average Local Preference Values for Communities:\")\nfor community, local_prefs in community_to_local_pref.items():\n    avg_local_pref = sum(local_prefs) / len(local_prefs)\n    print(f\"{community}: {avg_local_pref}\")"}], "is_classification": false}
{"id": "seed_task_3", "name": "Analyzing BGP Community Usage Trends", "instruction": "Analyze BGP community usage trends using data from collector rrc02 on June 1, 2021, between 00:00 and 01:00. Calculate the frequency of each BGP community and identify the top 5 most commonly used communities.", "instances": [{"input": "Analyzing BGP community usage trends using data from collector rrc02 on June 1, 2021, between 00:00 and 01:00.", "output": "import pybgpstream\nfrom collections import Counter\n\n# Initialize BGP Stream\nstream = pybgpstream.BGPStream(\n    from_time=\"2021-06-01 00:00:00\", until_time=\"2021-06-01 01:00:00\",\n    project=\"route-views\",\n    collectors=\"rrc02\",\n    record_type=\"updates\",\n    filter=\"ipversion 4\"\n)\n\n# Counter for BGP communities\ncommunities_counter = Counter()\n\n# Loop through the stream records\nfor rec in stream.records():\n    for elem in rec:\n        if elem.type == \"A\":\n            communities = elem.fields.get('communities', None)\n            if communities:\n                for community in communities.split(\",\"):\n                    communities_counter[community] += 1\n\n# Print the top 5 most commonly used communities\nprint(\"Top 5 Most Commonly Used Communities:\")\nfor comm, count in communities_counter.most_common(5):\n    print(f\"{comm}: {count}\")"}], "is_classification": false}
{"id": "seed_task_4", "name": "Identifying AS Path Prepending Usage", "instruction": "Identify Autonomous Systems (ASes) that potentially employ AS Path Prepending based on BGP data from collector route-views2 on January 15, 2021, between 00:00 and 01:00. Count and list ASes that engage in AS Path Prepending, along with the number of prepend events detected.", "instances": [{"input": "Identifying ASes employing AS Path Prepending based on BGP data from collector route-views2 on January 15, 2021, between 00:00 and 01:00.", "output": "import pybgpstream\nfrom collections import defaultdict\n\n# Initialize BGP Stream\nstream = pybgpstream.BGPStream(\n    from_time=\"2021-01-15 00:00:00\", until_time=\"2021-01-15 01:00:00\",\n    project=\"route-views\",\n    collectors=\"route-views2\",\n    record_type=\"updates\",\n    filter=\"ipversion 4\"\n)\n\n# Store potential AS Path Prepending events\nas_path_prepend_events = defaultdict(int)\n\n# Loop through the stream records\nfor rec in stream.records():\n    for elem in rec:\n        if elem.type == \"A\":\n            as_path = elem.fields.get('as-path', '').split()\n            as_count = defaultdict(int)\n\n            for as_number in as_path:\n                as_count[as_number] += 1\n\n            for as_number, count in as_count.items():\n                if count > 1:\n                    as_path_prepend_events[as_number] += 1\n\n# Display ASes employing AS Path Prepending\nprint(\"AS Employing Path Prepending:\")\nfor as_number, count in as_path_prepend_events.items():\n    print(f\"AS{as_number}: {count} prepend events\")"}], "is_classification": false}
{"id": "seed_task_5", "name": "Analyzing Prefix Announcements and Withdrawals", "instruction": "Analyze prefix announcements and withdrawals using BGP data from collector rrc04 on February 25, 2021, between 10:00 and 11:00. Count the frequency of announcements and withdrawals for each prefix and identify the top 5 most announced and withdrawn prefixes. Generate a report displaying the top 5 prefixes for both announcements and withdrawals along with their corresponding counts.", "instances": [{"input": "Analyzing prefix announcements and withdrawals using BGP data from collector rrc04 on February 25, 2021, between 10:00 and 11:00.", "output": "import pybgpstream\nfrom collections import Counter\n\n# Initialize BGP Stream\nstream = pybgpstream.BGPStream(\n    from_time=\"2021-02-25 10:00:00\", until_time=\"2021-02-25 11:00:00\",\n    project=\"ris\",\n    collectors=\"rrc04\",\n    record_type=\"updates\",\n    filter=\"ipversion 4\"\n)\n\n# Counters for announcements and withdrawals\nannounce_counter = Counter()\nwithdraw_counter = Counter()\n\n# Loop through the stream records\nfor rec in stream.records():\n    for elem in rec:\n        prefix = elem.fields.get('prefix', None)\n        if elem.type == \"A\":\n            announce_counter[prefix] += 1\n        elif elem.type == \"W\":\n            withdraw_counter[prefix] += 1\n\n# Print the top 5 most announced prefixes\nprint(\"Top 5 Most Announced Prefixes:\")\nfor prefix, count in announce_counter.most_common(5):\n    print(f\"{prefix}: {count} announcements\")\n\nprint(\"\\nTop 5 Most Withdrawn Prefixes:\")\nfor prefix, count in withdraw_counter.most_common(5):\n    print(f\"{prefix}: {count} withdrawals\")"}], "is_classification": false}
{"id": "seed_task_6", "name": "Identifying AS Peer Relationships", "instruction": "Identify Autonomous Systems (ASes) and their corresponding peer relationships using BGP data from collector route-views2 on May 15, 2021, between 00:00 and 01:00. Extract AS path information and determine which ASes are peers for each AS. Create a report listing the AS numbers along with their peers.", "instances": [{"input": "Identifying AS and their peer relationships using BGP data from collector route-views2 on May 15, 2021, between 00:00 and 01:00.", "output": "import pybgpstream\nfrom collections import defaultdict\n\n# Initialize BGP Stream\nstream = pybgpstream.BGPStream(\n    from_time=\"2021-05-15 00:00:00\", until_time=\"2021-05-15 01:00:00\",\n    project=\"route-views\",\n    collectors=\"route-views2\",\n    record_type=\"updates\",\n    filter=\"ipversion 4\"\n)\n\n# Store AS Peer relationships\nas_peers = defaultdict(set)\n\n# Loop through the stream records\nfor rec in stream.records():\n    for elem in rec:\n        if elem.type == \"A\":\n            as_path = elem.fields.get('as-path', '').split()\n            if len(as_path) > 1:\n                as_peers[as_path[-1]].add(as_path[-2])\n\n# Print ASes and their Peers\nprint(\"AS Peer Relationships:\")\nfor as_number, peers in as_peers.items():\n    print(f\"AS{as_number} has peers {', '.join(peers)}\")"}], "is_classification": false}
{"id": "seed_task_7", "name": "Identifying Top Hub ASes in BGP Paths", "instruction": "Identify the top hub Autonomous Systems (ASes) in BGP paths using data from collector rrc04 on July 20, 2021, between 10:00 and 11:00. Analyze AS path information and count the occurrence of each AS in the path to determine their hub status. Generate a report listing the top 5 hub ASes along with the number of times they appear in BGP paths.", "instances": [{"input": "Identifying top hub ASes in BGP paths using data from collector rrc04 on July 20, 2021, between 10:00 and 11:00.", "output": "import pybgpstream\nfrom collections import Counter\n\n# Initialize BGP Stream\nstream = pybgpstream.BGPStream(\n    from_time=\"2021-07-20 10:00:00\", until_time=\"2021-07-20 11:00:00\",\n    project=\"ris\",\n    collectors=\"rrc04\",\n    record_type=\"updates\",\n    filter=\"ipversion 4\"\n)\n\n# Counter for AS hub\nas_hub_counter = Counter()\n\n# Loop through the stream records\nfor rec in stream.records():\n    for elem in rec:\n        if elem.type == \"A\":\n            as_path = elem.fields.get('as-path', '').split()\n            as_hub_counter.update(as_path)\n\n# Print top 5 hub ASes\nprint(\"Top 5 Hub ASes:\")\nfor as_number, count in as_hub_counter.most_common(5):\n    print(f\"AS{as_number}: appears {count} times\")"}], "is_classification": false}
{"id": "seed_task_8", "name": "Analyzing Historical Trends in Unique Prefix Counts per Community","instruction": "Collect data for one-hour intervals over a period of three hours between 7:00 AM and 10:00 AM on 10th August 2015. For each one-hour interval, count the number of unique prefixes that are announced for each BGP community. Generate a summary of historical trends that shows how the number of unique prefixes for each community evolves over time.","instances": [    {        "input": "This script collects data for one-hour intervals over a period of three hours between 7:00 AM and 10:00 AM on 10th August 2015. For each one-hour interval, it counts the number of unique prefixes announced for each BGP community.",        "output": "import pybgpstream\nfrom collections import defaultdict\nfrom datetime import datetime, timedelta\n\n# Initialize the time variables\nstart_time = datetime.strptime(\"2015-08-10 07:00:00\", \"%Y-%m-%d %H:%M:%S\")\nend_time = datetime.strptime(\"2015-08-10 10:00:00\", \"%Y-%m-%d %H:%M:%S\")\ntime_increment = timedelta(hours=1)\n\n# Data structures for storing prefix counts\nhistorical_prefix_counts = defaultdict(lambda: defaultdict(int))\n\nwhile start_time < end_time:\n    from_time_str = start_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    until_time_str = (start_time + time_increment).strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    stream = pybgpstream.BGPStream(\n        from_time=from_time_str, until_time=until_time_str,\n        collectors=[\"rrc06\"],\n        record_type=\"ribs\",\n        filter=\"peer 25152 and prefix more 185.84.166.0/23 and community *:3400\"\n    )\n\n    current_prefix_count = defaultdict(int)\n    for rec in stream.records():\n        for elem in rec:\n            pfx = elem.fields['prefix']\n            communities = elem.fields['communities']\n            for community in communities:\n                current_prefix_count[community] += 1\n\n    # Save the data in the historical trends dictionary\n    for community, count in current_prefix_count.items():\n        historical_prefix_counts[community][from_time_str] = count\n\n    # Move to the next time slot\n    start_time += time_increment\n\n# Print the historical trends\nprint(\"Historical Trends in Unique Prefix Counts Per Community\")\nprint(\"-------------------------------------------------------\")\nfor community, time_series in historical_prefix_counts.items():\n    print(f\"Community: {community}\")\n    for timestamp, count in time_series.items():\n        print(f\"  {timestamp} -> {count} unique prefixes\")"}],"is_classification": false}
{"id": "seed_task_9", "name": "Detecting Withdrawal Spikes for Prefix '192.0.2.0/24'", "instruction": "Detect withdrawal spikes for the prefix '192.0.2.0/24' during May 5, 2022, from 12 PM to 8 PM. Identify time windows where the number of withdrawals exceeds 10 and print out the timestamps.", "instances": [{"input": "Detecting withdrawal spikes for the prefix '192.0.2.0/24' during May 5, 2022, from 12 PM to 8 PM.", "output": "from datetime import datetime\nimport pybgpstream\n\nstream = pybgpstream.BGPStream(\n    from_time=\"2022-05-05 12:00:00\", until_time=\"2022-05-05 20:00:00\",\n    record_type=\"updates\",\n    filter='prefix exact 192.0.2.0/24'\n)\n\nwithdrawal_threshold = 10\nwithdrawal_window = 600  # 10 minutes in seconds\n\nwithdrawal_count = 0\ncurrent_window_start = None\n\nfor elem in stream:\n    if elem.type == 'W':\n        if current_window_start is None:\n            current_window_start = elem.time\n        withdrawal_count += 1\n\n    if current_window_start and elem.time - current_window_start >= withdrawal_window:\n        if withdrawal_count > withdrawal_threshold:\n            print(f\"Withdrawal spike detected in window starting at {datetime.utcfromtimestamp(current_window_start).strftime('%Y-%m-%d %H:%M:%S')}\")\n        withdrawal_count = 0\n        current_window_start = None"}], "is_classification": false}
{"id": "seed_task_10", "name": "Analyzing BGP Update Events from Collector 'rrc00'", "instruction": "Investigate BGP update events originating from the 'rrc00' collector, with the goal of analyzing patterns in announcement (A) messages over a specified period. Specifically, focus on announcements with AS path containing '65000' and '65500', within the timeframe from September 1, 2023, 8:00 AM to September 3, 2023, 11:59 PM. Identify trends and visualize the time distribution of these announcements.", "instances": [{"input": "Investigating BGP update events from the 'rrc00' collector, focusing on announcements with AS path containing '65000' and '65500', within the timeframe from September 1, 2023, 8:00 AM to September 3, 2023, 11:59 PM.", "output": "import pybgpstream\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\nstream = pybgpstream.BGPStream(\n    project=\"routeviews\", collectors=\"rrc00\",\n    from_time=\"2023-09-01 08:00:00\", until_time=\"2023-09-03 23:59:59\",\n    record_type=\"updates\",\n    filter='path \"65000 65500\"'\n)\n\nannouncement_times = []\n\nfor elem in stream:\n    if elem.type == 'A':\n        announcement_times.append(elem.time)\n    print(datetime.utcfromtimestamp(elem.time).strftime('%Y-%m-%d %H:%M:%S'))\n\nprint(f\"Total number of announcements: {len(announcement_times)}\")\n\nif announcement_times:\n    time_bins = [i for i in range(int(min(announcement_times)), int(max(announcement_times)) + 3600, 3600)]\n    hist = plt.hist(announcement_times, bins=time_bins)\n\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Number of Announcements\")\n    plt.title(\"Time Distribution of Announcements\")\n    plt.show()"}], "is_classification": false}
{"id": "seed_task_11", "name": "Analyzing Withdrawn IPv6 Prefixes from 'rrc14' Collector", "instruction": "Retrieve BGP update events related to IPv6 prefixes originating from 'rrc14' collector during May 20, 2023, from 12:00 PM to 6:00 PM. Filter for withdrawals and identify the specific prefixes that have been withdrawn during this period. Present the prefixes in a readable format.", "instances": [{"input": "Retrieving BGP update events related to IPv6 prefixes originating from 'rrc14' collector during May 20, 2023, from 12:00 PM to 6:00 PM.", "output": "import pybgpstream\n\nstream = pybgpstream.BGPStream(\n    project=\"routeviews\", collectors=\"rrc14\",\n    from_time=\"2023-05-20 12:00:00\", until_time=\"2023-05-20 18:00:00\",\n    record_type=\"updates\",\n    filter='ipv 6'\n)\n\nwithdrawn_prefixes = set()\n\nfor elem in stream:\n    if elem.type == 'W':\n        withdrawn_prefixes.add(elem.fields[\"prefix\"])\n\nprint(\"Withdrawn IPv6 Prefixes:\")\nfor prefix in withdrawn_prefixes:\n    print(prefix)"}], "is_classification": false}
{"id": "seed_task_12", "name": "Analyzing Routing Changes for '192.168' Prefixes from 'rrc22' Collector", "instruction": "Collect BGP update events from 'rrc22' collector and analyze changes in routing information for prefixes starting with '192.168'. Focus on the time range between July 10, 2023, 6:00 AM and July 11, 2023, 11:59 PM. Determine the most frequent AS path associated with announcements during this period.", "instances": [{"input": "Collecting BGP update events from 'rrc22' collector for prefixes starting with '192.168' during July 10, 2023, 6:00 AM to July 11, 2023, 11:59 PM.", "output": "import pybgpstream\nfrom collections import Counter\n\nstream = pybgpstream.BGPStream(\n    project=\"ris\",\n    collectors=\"rrc22\",\n    from_time=\"2023-07-10 06:00:00 UTC\",\n    until_time=\"2023-07-11 23:59:59 UTC\",\n    record_type=\"updates\",\n)\n\nannouncement_as_paths = []\n\nfor elem in stream:\n    if elem.type == \"A\":\n        prefix = elem.fields.get(\"prefix\")\n        if prefix and prefix.startswith(\"192.168\"):\n            as_path = elem.fields.get(\"as-path\")\n            if as_path:\n                announcement_as_paths.append(as_path)\n\nmost_common_as_path = Counter(announcement_as_paths).most_common(1)\n\nif most_common_as_path:\n    print(f\"The most frequent AS path for prefixes starting with 192.168 is: {most_common_as_path[0][0]}\")\nelse:\n    print(\"No relevant announcements found.\")"}], "is_classification": false}
{"id": "seed_task_13", "name": "Exploring BGP Announcements with Community '65001:123'", "instruction": "Explore BGP update events from the 'rrc00' and 'rrc04' collectors, focusing on announcements involving the community value '65001:123'. The analysis should cover the period from April 1, 2023, 12:00 PM to April 5, 2023, 11:59 PM. Identify the unique ASes that originated these announcements and list them.", "instances": [{"input": "Exploring BGP update events from 'rrc00' and 'rrc04' collectors for announcements with community '65001:123' during April 1, 2023, 12:00 PM to April 5, 2023, 11:59 PM.", "output": "import pybgpstream\n\nstream = pybgpstream.BGPStream(\n    project=\"routeviews\", collectors=[\"rrc00\", \"rrc04\"],\n    from_time=\"2023-04-01 12:00:00\", until_time=\"2023-04-05 23:59:59\",\n    record_type=\"updates\",\n    filter='comm \"65001:123\"'\n)\n\noriginating_asns = set()\n\nfor elem in stream:\n    if elem.type == 'A':\n        originating_asns.add(elem.fields[\"as-path\"].split()[-1])\n\nprint(\"Unique ASes originating announcements with community '65001:123':\")\nfor asn in originating_asns:\n    print(asn)"}], "is_classification": false}
{"id": "seed_task_14", "name": "Analyzing BGP Announcements with AS Path '3356' and Prefix Length 24", "instruction": "Query BGP update events during June 15, 2023, from the 'rrc10' collector, focusing on announcements where the AS path includes AS 3356 and there's a prefix length of 24. Analyze the frequency of announcements per peer IP address during this time frame.", "instances": [{"input": "Query BGP update events from the 'rrc10' collector for announcements with AS path including '3356' and prefix length 24 during June 15, 2023. Analyze the frequency of announcements per peer IP address.", "output": "import pybgpstream\nfrom collections import Counter\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n    project=\"ris\",\n    collectors=\"rrc10\",\n    from_time=\"2023-06-15 00:00:00 UTC\",\n    until_time=\"2023-06-15 23:59:59 UTC\",\n    record_type=\"updates\",\n)\n\n# Store the peer IP addresses for announcements\npeer_ips = []\n\n# Loop over the stream of BGP records\nfor elem in stream:\n    # Check if the record is an announcement ('A')\n    if elem.type == 'A':\n        as_path = elem.fields.get('as-path', '')\n        prefix = elem.fields.get('prefix', '')\n        \n        # Check if the AS path includes AS 3356 and the prefix length is 24\n        if '3356' in as_path.split() and prefix:\n            prefix_length = int(prefix.split('/')[-1])  # Extract length from 'X.X.X.X/length'\n            if prefix_length == 24:\n                peer_ip = elem.peer_address\n                if peer_ip:\n                    peer_ips.append(peer_ip)\n\n# Count the frequency of announcements per peer IP\npeer_ip_count = Counter(peer_ips)\n\n# Print the frequency of announcements per peer IP\nfor peer_ip, count in peer_ip_count.items():\n    print(f\"Peer IP: {peer_ip}, Number of Announcements: {count}\")\n\n# If there are no relevant announcements\nif not peer_ip_count:\n    print(\"No relevant announcements found.\")"}], "is_classification": false}
{"id": "seed_task_15", "name": "Analyzing IPv6 Withdrawals from Multiple BGP Collectors", "instruction": "Describe the changes in network reachability from July 1, 2023, to July 7, 2023, using BGP updates collected from both the 'rrc01' and 'route-views' collectors. Focus on the withdrawals of IPv6 prefixes and analyze the frequency of these withdrawals per collector.", "instances": [{"input": "Analyze BGP updates collected from 'rrc01' and 'route-views' collectors for withdrawals of IPv6 prefixes during the period from July 1, 2023, to July 7, 2023. Analyze the frequency of withdrawals per collector.", "output": "import pybgpstream\nfrom collections import Counter\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n    project=\"ris,route-views\",\n    collectors=\"rrc01,route-views\",\n    from_time=\"2023-07-01 00:00:00 UTC\",\n    until_time=\"2023-07-07 23:59:59 UTC\",\n    record_type=\"updates\",\n)\n\n# Store collector-specific IPv6 withdrawals\nwithdrawals_per_collector = {}\n\n# Loop over the stream of BGP records\nfor elem in stream:\n    # Check if the record is a withdrawal ('W') and IPv6\n    if elem.type == 'W' and elem.fields.get('ip-version') == '6':\n        collector = elem.collector\n        if collector not in withdrawals_per_collector:\n            withdrawals_per_collector[collector] = 0\n        withdrawals_per_collector[collector] += 1\n\n# Print the frequency of IPv6 withdrawals per collector\nfor collector, count in withdrawals_per_collector.items():\n    print(f\"Collector: {collector}, Number of IPv6 Withdrawals: {count}\")\n\n# If there are no withdrawals\nif not withdrawals_per_collector:\n    print(\"No IPv6 withdrawals found.\")"}], "is_classification": false}
{"id": "seed_task_16", "name": "Analyzing Frequent AS Paths in IPv4 Announcements", "instruction": "Investigate routing changes between June 1, 2023, and June 30, 2023, collected from the 'rrc03' collector. Identify the top 5 most frequent AS paths in announcements for IPv4 prefixes. Calculate the percentage of these top AS paths compared to the total number of announcements.", "instances": [{"input": "Analyze BGP updates collected from the 'rrc03' collector for announcements of IPv4 prefixes between June 1, 2023, and June 30, 2023. Identify the top 5 most frequent AS paths and calculate their percentages compared to the total number of announcements.", "output": "import pybgpstream\nfrom collections import Counter\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n    project=\"ris\",\n    collectors=\"rrc03\",\n    from_time=\"2023-06-01 00:00:00 UTC\",\n    until_time=\"2023-06-30 23:59:59 UTC\",\n    record_type=\"updates\",\n)\n\n# Store AS paths for IPv4 announcements\nas_paths = []\n\n# Loop over the stream of BGP records\nfor elem in stream:\n    # Check if the record is an announcement ('A') and IPv4\n    if elem.type == 'A' and elem.fields.get('ip-version') == '4':\n        as_path = elem.fields.get('as-path', '')\n        as_paths.append(as_path)\n\n# Count the frequency of AS paths in announcements\nas_path_count = Counter(as_paths)\n\n# Get the top 5 most frequent AS paths\ntop_as_paths = as_path_count.most_common(5)\n\n# Calculate the total number of announcements\ntotal_announcements = len(as_paths)\n\n# Print the top AS paths and their frequencies\nfor as_path, count in top_as_paths:\n    percentage = (count / total_announcements) * 100\n    print(f\"AS Path: {as_path}, Frequency: {count}, Percentage: {percentage:.2f}%\")"}], "is_classification": false}
{"id": "seed_task_17", "name": "Analyzing Increase in Announcements for Different Prefix Lengths", "instruction": "Retrieve BGP update events during August 15, 2023, from the 'rrc04' collector. Identify announcements for prefixes with a length of exactly 16 and 24. Determine the percentage increase in the number of announcements for prefixes with a length of 24 compared to those with a length of 16.", "instances": [{"input": "Retrieve BGP update events from the 'rrc04' collector for August 15, 2023. Identify announcements for prefixes with a length of 16 and 24. Calculate the percentage increase in the number of announcements for prefixes with length 24 compared to those with length 16.", "output": "import pybgpstream\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n    project=\"ris\",\n    collectors=\"rrc04\",\n    from_time=\"2023-08-15 00:00:00 UTC\",\n    until_time=\"2023-08-15 23:59:59 UTC\",\n    record_type=\"updates\",\n)\n\n# Store counts for prefixes with length 16 and 24\ncount_16 = 0\ncount_24 = 0\n\n# Loop over the stream of BGP records\nfor elem in stream:\n    # Check if the record is an announcement ('A')\n    if elem.type == 'A':\n        prefix = elem.fields.get('prefix', '')\n        if prefix:\n            prefix_length = int(prefix.split('/')[-1])  # Extract length from 'X.X.X.X/length'\n            if prefix_length == 16:\n                count_16 += 1\n            elif prefix_length == 24:\n                count_24 += 1\n\n# Calculate the percentage increase\nif count_16 > 0:\n    percentage_increase = ((count_24 - count_16) / count_16) * 100\nelse:\n    percentage_increase = 0\n\nprint(f\"Percentage Increase in Announcements (Prefix Length 16 to 24): {percentage_increase:.2f}%\")"}], "is_classification": false}
{"id": "seed_task_18", "name": "Analyzing Average Withdrawals per Day with AS 2914 in AS Path", "instruction": "Analyze BGP updates between September 1, 2023, and September 15, 2023, collected from the 'rrc05' collector. Focus on withdrawals with the AS path containing at least one instance of AS 2914. Calculate the average number of withdrawals per day during this period.", "instances": [{"input": "Analyze BGP updates from the 'rrc05' collector for withdrawals with AS path containing AS 2914. Focus on the period between September 1, 2023, and September 15, 2023. Calculate the average number of withdrawals per day during this period.", "output": "import pybgpstream\nfrom datetime import datetime, timedelta\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n    project=\"ris\",\n    collectors=\"rrc05\",\n    from_time=\"2023-09-01 00:00:00 UTC\",\n    until_time=\"2023-09-15 23:59:59 UTC\",\n    record_type=\"updates\",\n)\n\n# Store withdrawal count and date information\nwithdrawal_count = 0\ncurrent_date = None\nday_counter = 0\n\n# Loop over the stream of BGP records\nfor elem in stream:\n    # Check if the record is a withdrawal ('W')\n    if elem.type == 'W':\n        as_path = elem.fields.get('as-path', '')\n        if '2914' in as_path.split():\n            withdrawal_count += 1\n\n        # Check if the date has changed\n        record_date = datetime.utcfromtimestamp(elem.time)\n        if current_date is None:\n            current_date = record_date\n        elif record_date.date() != current_date.date():\n            day_counter += 1\n            current_date = record_date\n\n# Calculate the average number of withdrawals per day\naverage_withdrawals_per_day = withdrawal_count / (day_counter + 1)\n\nprint(f\"Average Withdrawals per Day: {average_withdrawals_per_day:.2f}\")"}], "is_classification": false}
{"id": "seed_task_19", "name": "Analyzing Announcements with Specific Community Attribute", "instruction": "Collect BGP updates during October 10, 2023, from the 'rrc06' collector. Identify announcements with the community attribute containing '64500:200'. Calculate the percentage of these announcements compared to the total number of announcements during this day.", "instances": [{"input": "Collect BGP updates from the 'rrc06' collector for October 10, 2023. Identify announcements with the community attribute '64500:200'. Calculate the percentage of these announcements compared to the total number of announcements on that day.", "output": "import pybgpstream\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n    project=\"ris\",\n    collectors=\"rrc06\",\n    from_time=\"2023-10-10 00:00:00 UTC\",\n    until_time=\"2023-10-10 23:59:59 UTC\",\n    record_type=\"updates\",\n)\n\n# Store counts for announcements with the specific community attribute\ncount_community_64500_200 = 0\ntotal_announcements = 0\n\n# Loop over the stream of BGP records\nfor elem in stream:\n    # Check if the record is an announcement ('A')\n    if elem.type == 'A':\n        total_announcements += 1\n        community = elem.fields.get('communities', [])\n        if '64500:200' in community:\n            count_community_64500_200 += 1\n\n# Calculate the percentage of announcements with the specific community attribute\nif total_announcements > 0:\n    percentage_with_community = (count_community_64500_200 / total_announcements) * 100\nelse:\n    percentage_with_community = 0\n\nprint(f\"Percentage of Announcements with Community 64500:200: {percentage_with_community:.2f}%\")"}], "is_classification": false}
{"id": "seed_task_20", "name": "Analyzing MOAS Prefixes in BGP Update Records","instruction": "Retrieve BGP update records from the 'rrc01' collector for the time period between 2023-06-15 12:00:00 and 2023-06-15 14:00:00. Identify and extract MOAS prefixes during this time frame to explore potential multi-homing cases. Present the MOAS prefixes along with their corresponding AS path information.","instances": [{"input": "Retrieve BGP update records from the 'rrc01' collector between 2023-06-15 12:00:00 and 2023-06-15 14:00:00. Extract and identify MOAS prefixes during this period along with their AS path.",        "output": "Here are the identified MOAS prefixes and their corresponding AS paths:\nfrom collections import defaultdict\nimport pybgpstream\n\n# Configure BGPStream\nstream = pybgpstream.BGPStream(\n    from_time=\"2023-06-15 12:00:00\",\n    until_time=\"2023-06-15 14:00:00\",\n    collectors=[\"rrc01\"],\n    record_type=\"updates\"\n)\n\n# Dictionary to store prefix and its AS path\nprefix_aspath = defaultdict(list)\n\n# Fetch records and populate dictionary\nfor rec in stream.records():\n    for elem in rec:\n        if 'communities' in elem.fields and 'as-path' in elem.fields:\n            pfx = elem.fields['prefix']\n            as_path = elem.fields['as-path'].split(' ')\n            prefix_aspath[pfx] = as_path\n\n# Analyze MOAS prefixes\nmoas_prefixes = {}\n\nfor pfx, as_path in prefix_aspath.items():\n    if len(as_path) > 1:\n        moas_prefixes[pfx] = as_path\n\n# Display MOAS prefixes and AS paths\nprint('MOAS prefixes and their AS paths:')\nfor pfx, as_path in moas_prefixes.items():\n    print(f'Prefix: {pfx}, AS Path: {', '.join(as_path)}')\n"    }],"is_classification": false}
{"id": "seed_task_21", "name": "Analyzing Prefixes with Multiple AS Path Variations", "instruction": "Retrieve BGP RIB records from the 'rrc02' and 'route-views.sg' collectors for the time period between 2023-09-15 09:00:00 and 2023-09-15 12:00:00. Create a dictionary to store prefixes and their corresponding distinct AS path variations. Identify prefixes with multiple AS path variations (potential MOAS). Display these prefixes along with their distinct AS path variations.", "instances": [{"input": "Retrieve BGP RIB records from 'rrc02' and 'route-views.sg' collectors between 2023-09-15 09:00:00 and 2023-09-15 12:00:00. Create a dictionary to store prefixes and their distinct AS path variations. Identify prefixes with multiple AS path variations (potential MOAS).", "output": "Here are the prefixes with multiple AS path variations:\n\nfrom collections import defaultdict\nimport pybgpstream\n\nstream = pybgpstream.BGPStream(from_time=\"2023-09-15 09:00:00\", until_time=\"2023-09-15 12:00:00\", collectors=[\"rrc02\", \"route-views.sg\"], record_type=\"ribs\")\n\nprefix_as_paths = defaultdict(set)\n\nfor rec in stream.records():\n    for elem in rec:\n        pfx = elem.fields.get(\"prefix\")\n        as_path = elem.fields.get(\"as-path\")\n        if pfx and as_path:\n            ases = as_path.split(\" \")\n            prefix_as_paths[pfx].add(tuple(ases))\n\nmoas_prefixes = {pfx: paths for pfx, paths in prefix_as_paths.items() if len(paths) > 1}\n\nfor pfx, paths in moas_prefixes.items():\n    print(f\"Prefix: {pfx}, AS Path Variations: {paths}\")"}], "is_classification": false}
{"id": "seed_task_22", "name": "Geographical Analysis of MOAS Prefixes", "instruction": "Perform a geographical analysis on MOAS (Multiple Origin AS) prefixes to identify the geographical regions from which these prefixes are being announced. Time Frame: Use the BGP data from August 1, 2019, between 14:00:00 and 14:30:00 UTC. Collectors: Use rrc06 as the data collector. Data Type: Focus on BGP update messages. Geographical Data: Utilize the GeoLite2 database to fetch the geographical location of the origin ASes.", "instances": [{"input": "Perform geographical analysis on MOAS prefixes using BGP data from August 1, 2019, 14:00:00 to 14:30:00 UTC, collected by rrc06. Utilize GeoLite2 database for AS geographical locations.", "output": "{\"prefixes\": [\n{\"prefix\": \"192.0.2.0/24\", \"origin_as\": [\n{\"as\": \"AS12345\", \"countries\": [\"US\"]},\n{\"as\": \"AS54321\", \"countries\": [\"CA\"]}\n]},\n{\"prefix\": \"198.51.100.0/24\", \"origin_as\": [\n{\"as\": \"AS98765\", \"countries\": [\"DE\"]},\n{\"as\": \"AS54321\", \"countries\": [\"CA\"]}\n]}\n]}" }], "is_classification": false}
{"id": "seed_task_23", "name": "Time-Series Analysis of MOAS Events", "instruction": "Perform time-series analysis on the MOAS events from the 'rrc00' collector between 7:50 am and 8:10 am UTC on August 1, 2015. Identify the timestamps when MOAS events are observed and the frequency of these events during this period. Plot this information on a time-series graph.", "instances": [{"input": "Perform time-series analysis on MOAS events from 'rrc00' collector between 7:50 am and 8:10 am UTC on August 1, 2015. Identify timestamps of MOAS events and their frequencies. Plot on a time-series graph.", "output": "Here's the time-series analysis of MOAS events:\n\nfrom collections import defaultdict\nimport pybgpstream\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\n# Initialize Time-Series dict\ntime_series_data = defaultdict(int)\n\n# Initialize the BGPStream\nstream = pybgpstream.BGPStream(\n    from_time=\"2015-08-01 07:50:00\", until_time=\"2015-08-01 08:10:00\",\n    collectors=[\"rrc00\"],\n    record_type=\"updates\",\n)\n\n# Fetching BGP data and updating time-series\nfor rec in stream.records():\n    timestamp = datetime.utcfromtimestamp(rec.time).strftime('%Y-%m-%d %H:%M:%S')\n    time_series_data[timestamp] += 1\n\n# Plotting time-series data\nplt.plot(list(time_series_data.keys()), list(time_series_data.values()))\nplt.xlabel('Time')\nplt.ylabel('Number of MOAS Events')\nplt.show()"}], "is_classification": false}
{"id": "seed_task_24", "name": "Security Analysis for BGP Hijacking Attempts", "instruction": "Conduct a security analysis focused on identifying potential BGP hijacking attempts. Use data from the 'rrc00' collector for the time window between 7:50 am and 8:10 am UTC on August 1, 2015. Specifically, look for MOAS prefixes where multiple Origin ASes are involved. List out any suspicious Origin ASes for these MOAS prefixes as they could be potential hijack attempts.", "instances": [{"input": "Perform security analysis for potential BGP hijacking attempts using data from 'rrc00' collector between 7:50 am and 8:10 am UTC on August 1, 2015. Identify MOAS prefixes with multiple suspicious Origin ASes.", "output": "Here are the suspicious MOAS prefixes with potential hijack attempts:\n\nfrom collections import defaultdict\nimport pybgpstream\n\n# Initialize the BGPStream\nstream = pybgpstream.BGPStream(\n    from_time=\"2015-08-01 07:50:00\", until_time=\"2015-08-01 08:10:00\",\n    collectors=[\"rrc00\"],\n    record_type=\"updates\",\n)\n\n# Potential Hijack ASes\npotential_hijacks = defaultdict(set)\n\n# Process records\nfor rec in stream.records():\n    for elem in rec:\n        try:\n            pfx = elem.fields['prefix']\n            as_path = elem.fields['as-path'].split()\n            if as_path:\n                origin = as_path[-1]\n                potential_hijacks[pfx].add(origin)\n        except KeyError:\n            continue\n\n# Identify MOAS Prefixes with potential hijacks\nfor pfx, origins in potential_hijacks.items():\n    if len(origins) > 1:\n        print(f\"Suspicious MOAS Prefix: {pfx}, Origin ASes: {', '.join(origins)}\")"}], "is_classification": false}
{"id": "seed_task_25", "name": "Security Analysis for BGP Route Leaks Involving MOAS Prefixes", "instruction": "Conduct a security analysis to identify BGP route leaks involving MOAS prefixes. Use data from the 'rrc04' collector for the time window between 10:00 am and 10:20 am UTC on September 10, 2019. Specifically, identify MOAS prefixes where the multiple Origin ASes are not in a direct AS relationship (i.e., they are not provider-customer or peer-to-peer). List out the MOAS prefixes and any involved Origin ASes that could potentially be part of a route leak.", "instances": [{"input": "Perform security analysis to identify BGP route leaks involving MOAS prefixes using data from 'rrc04' collector between 10:00 am and 10:20 am UTC on September 10, 2019. Identify MOAS prefixes with non-direct AS relationships.", "output": "Here are the MOAS prefixes potentially involved in route leaks:\n\nfrom collections import defaultdict\nimport pybgpstream\n\n# Initialize the BGPStream\nstream = pybgpstream.BGPStream(\n    from_time=\"2019-09-10 10:00:00\", until_time=\"2019-09-10 10:20:00\",\n    collectors=[\"rrc04\"],\n    record_type=\"updates\",\n)\n\n# Dictionary to store potential route leaks\npotential_route_leaks = defaultdict(set)\n\n# Process records\nfor rec in stream.records():\n    for elem in rec:\n        try:\n            pfx = elem.fields['prefix']\n            as_path = elem.fields['as-path'].split()\n            if as_path:\n                origin = as_path[-1]\n                potential_route_leaks[pfx].add(origin)\n        except KeyError:\n            continue\n\n# Identify MOAS Prefixes with potential route leaks (this part would ideally use some AS relationship database)\nfor pfx, origins in potential_route_leaks.items():\n    if len(origins) > 1:\n        # Here you would actually check the AS relationships (Provider-Customer or Peer-to-Peer)\n        # For demonstration, we assume that these are not in a direct AS relationship\n        print(f\"Potential Route Leak MOAS Prefix: {pfx}, Origin ASes: {', '.join(origins)}\")"}], "is_classification": false}
{"id": "seed_task_26", "name": "AS Relationship Analysis for MOAS Prefixes", "instruction": "Determine the AS relationships involved in MOAS prefixes to understand if they belong to the same organization or different organizations. Use the 'rrc00' collector for data between 7:50 am and 8:10 am UTC on August 1, 2015. Filter based on AS path to include records that have at least one AS that is known to belong to a large organization (for example, AS 65501).", "instances": [{"input": "Perform AS relationship analysis for MOAS prefixes using data from 'rrc00' collector between 7:50 am and 8:10 am UTC on August 1, 2015. Filter based on AS path including AS 65501.", "output": "Here are the AS relationships for MOAS prefixes:\n\nfrom collections import defaultdict\nimport pybgpstream\n\n# Initialize stream for AS relationships\nstream = pybgpstream.BGPStream(\n    from_time=\"2015-08-01 07:50:00\", until_time=\"2015-08-01 08:10:00\",\n    collectors=[\"rrc00\"],\n    record_type=\"updates\",\n    filter=\"path _65501_\"\n)\n\n# Create a dictionary to store MOAS prefixes and their origin ASes\nmoas_as_relationships = defaultdict(set)\n\nfor rec in stream.records():\n    for elem in rec:\n        if 'communities' in elem.fields and 'as-path' in elem.fields:\n            prefix = elem.fields['prefix']\n            as_path = elem.fields['as-path'].split(\" \")\n            origin_as = as_path[-1]\n            moas_as_relationships[prefix].add(origin_as)\n\n# Output the MOAS prefixes and their origin ASes\nfor prefix in moas_as_relationships:\n    if len(moas_as_relationships[prefix]) > 1:\n        print(f\"{prefix} -> {moas_as_relationships[prefix]}\")"}], "is_classification": false}
{"id": "seed_task_27", "name": "Community and Policy Analysis for MOAS Events", "instruction": "Perform a community and policy analysis to understand the implications of MOAS (Multiple Origin AS) events on BGP policy decisions. Use data collected from the 'rrc00' collector between 7:50 am and 8:10 am UTC on August 1, 2015. Filter BGP records to only include IPv4 updates that carry community values (comm filter). Report on the BGP community values associated with MOAS prefixes and discuss the intent behind these community tags.", "instances": [{"input": "Perform community and policy analysis for MOAS events using data from 'rrc00' collector between 7:50 am and 8:10 am UTC on August 1, 2015. Filter IPv4 updates with community values.", "output": "Here are the MOAS prefixes with associated community values:\n\nfrom collections import defaultdict\nimport pybgpstream\n\n# Initialize stream for community and policy analysis\nstream = pybgpstream.BGPStream(\n    from_time=\"2015-08-01 07:50:00\", until_time=\"2015-08-01 08:10:00\",\n    collectors=[\"rrc00\"],\n    record_type=\"updates\",\n    filter=\"ipversion 4\"\n)\n\n# Create a dictionary to store MOAS prefixes and their community values\n# We'll use a list instead of a set for each prefix.\nmoas_communities = defaultdict(list)\n\nfor rec in stream.records():\n    for elem in rec:\n        if 'communities' in elem.fields and 'as-path' in elem.fields:\n            prefix = elem.fields['prefix']\n            community = elem.fields['communities']\n            as_path = elem.fields['as-path'].split(\" \")\n            origin_as = as_path[-1]\n            # Append the origin_as and community as a tuple to the list corresponding to each prefix\n            moas_communities[prefix].append((origin_as, community))\n\n# Output the MOAS prefixes along with their community values\nfor prefix in moas_communities:\n    if len(set(x[0] for x in moas_communities[prefix])) > 1:  # check the unique origin AS\n        print(f\"{prefix} -> {moas_communities[prefix]}\")"}], "is_classification": false}
{"id": "seed_task_28", "name": "In-Depth Path Length Analysis of BGP and Graph Representation", "instruction": "Perform an in-depth path length analysis to compare the shortest path lengths between peer and origin ASes in a real BGP environment and their respective shortest path lengths in an undirected graph representation. Use BGP RIBs collected from the 'rrc00' collector between 7:50:00 and 8:10:00 on August 1, 2015. Output the findings as a list of tuples containing (peer, origin, BGP_hop_length, shortest_graph_length).", "instances": [{"input": "Perform in-depth path length analysis to compare shortest path lengths between peer and origin ASes in a real BGP environment and undirected graph representation using BGP RIBs from 'rrc00' collector between 7:50:00 and 8:10:00 on August 1, 2015. Output findings as a list of tuples.", "output": "Here are the findings of the path length analysis:\n\nfrom collections import defaultdict\nfrom itertools import groupby\nimport networkx as nx\nimport pybgpstream\n\n# Initialize graph and BGP length map\nas_graph = nx.Graph()\nbgp_lens = defaultdict(lambda: defaultdict(lambda: None))\n\n# Initialize BGPStream\nstream = pybgpstream.BGPStream(\n    from_time=\"2015-08-01 07:50:00\", until_time=\"2015-08-01 08:10:00\",\n    collectors=[\"rrc00\"],\n    record_type=\"ribs\"\n)\n\n# Populate graph and BGP length map\nfor rec in stream.records():\n    for elem in rec:\n        peer = str(elem.peer_asn)\n        hops = [k for k, _ in groupby(elem.fields['as-path'].split(\" \"))]\n        if len(hops) > 1 and hops[0] == peer:\n            origin = hops[-1]\n            for i in range(len(hops) - 1):\n                as_graph.add_edge(hops[i], hops[i+1])\n            bgp_lens[peer][origin] = min(list(filter(bool, [bgp_lens[peer][origin], len(hops)])))\n\n# Compare BGP hop length to graph shortest path\nfor peer in bgp_lens:\n    for origin in bgp_lens[peer]:\n        nxlen = len(nx.shortest_path(as_graph, source=peer, target=origin))\n        print((peer, origin, bgp_lens[peer][origin], nxlen))"}], "is_classification": false}
{"id": "seed_task_29", "name": "AS Relationship Analysis for Path Inflation", "instruction": "Analyze how different AS relationships contribute to path inflation. Create a map that assigns relationships (Peer, Customer, Provider) between AS pairs. Utilize the same data set from rrc00 between 10:50 and 11:00 on April 1, 2021. Provide a summary output listing each AS pair, their relationship type, and whether the BGP path was inflated compared to the graph shortest path.", "instances": [{"input": "Analyze AS relationships' contribution to path inflation using data from 'rrc00' collector between 10:50 and 11:00 on April 1, 2021. Create a map with AS relationships (Peer, Customer, Provider). Provide a summary output listing each AS pair, their relationship type, and inflation status.", "output": "Here's the analysis of AS relationships and path inflation:\n\nfrom collections import defaultdict\nfrom itertools import groupby\nimport networkx as nx\nimport pybgpstream\n\n# Initialize graph and BGP length map\nas_graph = nx.Graph()\nbgp_lens = defaultdict(lambda: defaultdict(lambda: None))\n\n# Initialize AS relationship map\nas_relationships = defaultdict(lambda: defaultdict(str))\n\n# Hardcoded AS relationships for demonstration (you would usually populate this from a database or file)\nas_relationships[\"AS64500\"][\"AS64501\"] = \"Provider\"\nas_relationships[\"AS64501\"][\"AS64502\"] = \"Customer\"\nas_relationships[\"AS64502\"][\"AS64503\"] = \"Peer\"\n\n# Initialize BGPStream\nstream = pybgpstream.BGPStream(\n    from_time=\"2021-04-01 10:50:00\", until_time=\"2021-04-01 11:00:00\",\n    collectors=[\"rrc00\"],\n    record_type=\"ribs\"\n)\n\n# Populate graph and BGP length map\nfor rec in stream.records():\n    for elem in rec:\n        peer = str(elem.peer_asn)\n        hops = [k for k, _ in groupby(elem.fields['as-path'].split(\" \"))]\n        if len(hops) > 1 and hops[0] == peer:\n            origin = hops[-1]\n            for i in range(len(hops) - 1):\n                as_graph.add_edge(hops[i], hops[i + 1])\n            bgp_lens[peer][origin] = min(list(filter(bool, [bgp_lens[peer][origin], len(hops)])))\n\n# Initialize an output map to capture inflation due to relationship\nrelationship_inflation = defaultdict(set)\n\n# Assess the impact of relationships\nfor peer in bgp_lens:\n    for origin in bgp_lens[peer]:\n        try:\n            nxlen = len(nx.shortest_path(as_graph, source=peer, target=origin))\n            relationship = as_relationships[peer].get(origin, \"Unknown\")\n            inflated = \"Yes\" if bgp_lens[peer][origin] > nxlen else \"No\"\n\n            relationship_inflation[relationship].add((peer, origin, inflated))\n        except nx.NetworkXNoPath:\n            print(f\"No path exists in the graph between {peer} and {origin}. Skipping.\")\n\n# Print the result\nfor relationship in relationship_inflation:\n    print(f\"For relationship {relationship}, the following peer-origin pairs show inflation:\")\n    for peer, origin, inflated in relationship_inflation[relationship]:\n        print(f\"{peer} - {origin} : Inflated = {inflated}\")"}], "is_classification": false}
{"id": "seed_task_30", "name": "Advanced AS Relationship Analysis for Path Inflation", "instruction": "Analyze the effects of different AS relationships—Peer, Customer, Provider—on BGP path inflation. We'll use data collected from the rrc00 collector between 10:50 and 11:00 on April 1, 2021. Create a map that assigns the relationship between each AS pair, calculate the actual BGP path length and the graph-theoretical shortest path between each AS pair and output the list of each AS pair, their relationship type, and whether the BGP path is inflated compared to the graph's shortest path.", "instances": [{"input": "Analyze the effects of different AS relationships—Peer, Customer, Provider—on BGP path inflation using data from 'rrc00' collector between 10:50 and 11:00 on April 1, 2021. Create a map with AS relationships. Calculate BGP and graph-theoretical path lengths. Output each AS pair, their relationship type, and inflation status.", "output": "Here's the analysis of AS relationship effects on BGP path inflation:\n\nimport pybgpstream\nimport networkx as nx\nfrom collections import defaultdict\nfrom itertools import groupby\nfrom datetime import datetime\n\n# Create an instance of a simple undirected graph\nas_graph = nx.Graph()\n\n# Data structure to hold the shortest AS path lengths\nbgp_lens = defaultdict(lambda: defaultdict(lambda: None))\n\n# Data structure to hold time-based path lengths\ntime_based_bgp_lens = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: None)))\n\n# Simulated event timeline\nevent_timeline = defaultdict(list)\n\n# Initialize BGP Stream\nstream = pybgpstream.BGPStream(\n    from_time=\"2015-08-01 07:50:00\", until_time=\"2015-08-01 08:10:00\",\n    collectors=[\"rrc00\"],\n    record_type=\"ribs\",\n)\n\n# Populate the graph and path length data\nfor rec in stream.records():\n    timestamp = datetime.utcfromtimestamp(rec.time)\n    for elem in rec:\n        # Get the peer ASn\n        peer = str(elem.peer_asn)\n        \n        # Get the array of ASns in the AS path and remove repeatedly prepended ASns\n        hops = [k for k, g in groupby(elem.fields['as-path'].split(\" \"))]\n        \n        if len(hops) > 1 and hops[0] == peer:\n            # Get the origin ASn\n            origin = hops[-1]\n            \n            # Add new edges to the NetworkX graph\n            for i in range(0,len(hops)-1):\n                as_graph.add_edge(hops[i],hops[i+1])\n            \n            # Update the AS path length data\n            bgp_lens[peer][origin] = min(list(filter(bool,[bgp_lens[peer][origin],len(hops)])))\n            \n            # Update the time-based AS path length data\n            time_based_bgp_lens[timestamp][peer][origin] = \\\n                min(list(filter(bool,[time_based_bgp_lens[timestamp][peer][origin],len(hops)])))\n\n# Simulated event\nevent_timeline[\"2015-08-01 08:00:00\"].append(\"Outage at AS64501\")\n\n# Time-series analysis\nfor timestamp in sorted(time_based_bgp_lens.keys()):\n    total_inflation = 0\n    count = 0\n    \n    for peer in time_based_bgp_lens[timestamp]:\n        for origin in time_based_bgp_lens[timestamp][peer]:\n            try:\n                nxlen = len(nx.shortest_path(as_graph, peer, origin))\n                inflation = time_based_bgp_lens[timestamp][peer][origin] - nxlen\n                total_inflation += inflation\n                count += 1\n            except nx.NetworkXNoPath:\n                continue\n    \n    average_inflation = total_inflation / count if count > 0 else 0\n    \n    print(f\"Timestamp: {timestamp}, Average Path Inflation: {average_inflation}\")\n    \n    if timestamp.strftime(\"%Y-%m-%d %H:%M:%S\") in event_timeline:\n        for event in event_timeline[timestamp.strftime(\"%Y-%m-%d %H:%M:%S\")]:\n            print(f\"  -> Event: {event}, Possible Inflation: {average_inflation}\")"}], "is_classification": false}
{"id": "seed_task_31", "name": "Event Correlation: BGP Community Transitions","instruction": "Track how prefixes move between different BGP communities during 7:00 AM to 9:00 AM on 1st August 2018. Print out transitions between communities during each 10-minute time slice, along with the count of such transitions. Transitions should be printed if a prefix changes its community.","instances": [{"input": "Track how prefixes move between different BGP communities during 7:00 AM to 9:00 AM on 1st August 2018. Print out transitions between communities during each 10-minute time slice, along with the count of such transitions. Transitions should be printed if a prefix changes its community.",        "output": "import pybgpstream\nfrom collections import defaultdict, Counter\nfrom datetime import datetime, timedelta\n\n# Initialize the starting and ending time\nstart_time = datetime.strptime(\"2018-08-01 07:00:00\", \"%Y-%m-%d %H:%M:%S\")\nend_time = datetime.strptime(\"2018-08-01 09:00:00\", \"%Y-%m-%d %H:%M:%S\")\n\n# Define the time increment for each interval (10 minutes)\ntime_increment = timedelta(minutes=10)\n\n# Data structures\nprefix_community_history = defaultdict(str)\ncommunity_transitions = defaultdict(Counter)\n\nwhile start_time < end_time:\n    # Format the start and end time of each interval\n    from_time_str = start_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    until_time_str = (start_time + time_increment).strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    # Initialize BGPStream\n    stream = pybgpstream.BGPStream(\n        from_time=from_time_str, until_time=until_time_str,\n        collectors=[\"rrc06\"],\n        record_type=\"ribs\",\n        filter=\"peer 25152 and prefix more 185.84.166.0/23 and community *:3400\"\n    )\n\n    # Collect the data\n    for rec in stream.records():\n        for elem in rec:\n            pfx = elem.fields['prefix']\n            communities = elem.fields['communities']\n\n            # For simplification, let's consider only the first community if there are multiple\n            community = list(communities)[0] if communities else 'none'\n\n            # Check if the community has changed for this prefix\n            last_community = prefix_community_history.get(pfx, 'none')\n            if last_community != community:\n                community_transitions[last_community][community] += 1\n                prefix_community_history[pfx] = community\n\n    # Move to next time slot\n    start_time += time_increment\n\n# Output transitions as JSONL\nfor src_community, transitions in community_transitions.items():\n    for dst_community, count in transitions.items():\n        output = {\"source_community\": src_community, \"destination_community\": dst_community, \"transition_count\": count}\n        print(json.dumps(output))"}],"is_classification": false}
{"id": "seed_task_32", "name": "Monitoring Unique Prefixes for BGP Communities","instruction": "Monitor the number of unique prefixes for each BGP community between 7:00 AM and 9:00 AM on 10th August 2015. Trigger an alert if the number of unique prefixes for a community changes by more than 20% between two consecutive 10-minute intervals.","instances": [    {        "input": "This script monitors the number of unique prefixes for each BGP community between 7:00 AM and 9:00 AM on 10th August 2015. It triggers an alert if the number of unique prefixes for a community changes by more than 20% between two consecutive 10-minute intervals.",        "output": "import pybgpstream\nfrom collections import defaultdict\nfrom datetime import datetime, timedelta\n\n# Initialize the time variables\nstart_time = datetime.strptime(\"2015-08-10 07:00:00\", \"%Y-%m-%d %H:%M:%S\")\nend_time = datetime.strptime(\"2015-08-10 09:00:00\", \"%Y-%m-%d %H:%M:%S\")\ntime_increment = timedelta(minutes=10)\n\n# Data structures for storing prefix counts\nprev_prefix_count = defaultdict(int)\ncurrent_prefix_count = defaultdict(int)\n\nwhile start_time < end_time:\n    from_time_str = start_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    until_time_str = (start_time + time_increment).strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    stream = pybgpstream.BGPStream(\n        from_time=from_time_str, until_time=until_time_str,\n        collectors=[\"rrc06\"],\n        record_type=\"ribs\",\n        filter=\"peer 25152 and prefix more 185.84.166.0/23 and community *:3400\"\n    )\n\n    for rec in stream.records():\n        for elem in rec:\n            pfx = elem.fields['prefix']\n            communities = elem.fields['communities']\n            for community in communities:\n                current_prefix_count[community] += 1\n\n    # Check for alerts\n    for community in current_prefix_count.keys():\n        if community in prev_prefix_count:\n            prev_count = prev_prefix_count[community]\n            current_count = current_prefix_count[community]\n            percent_change = ((current_count - prev_count) / prev_count) * 100 if prev_count > 0 else 0\n\n            if abs(percent_change) > 20:\n                alert = {\"community\": community, \"prev_count\": prev_count, \"current_count\": current_count, \"percent_change\": percent_change}\n                print(json.dumps(alert))\n\n    # Update previous counts\n    prev_prefix_count = current_prefix_count.copy()\n    current_prefix_count.clear()\n\n    # Move to the next time slot\n    start_time += time_increment"}],"is_classification": false}
{"id": "seed_task_33", "name": "BGP Prefix and Community Analysis", "instruction": "Collect data for one-hour intervals over three hours between 7:00 AM and 10:00 AM on August 10, 2015. Count the number of unique prefixes announced for each BGP community. Generate a summary of historical trends showing how the number of unique prefixes for each community evolves over time.", "instances": [{"input": "Data collection and analysis for one-hour intervals over three hours.", "output": "Here are the historical trends for the number of unique prefixes for each BGP community:\n\nimport pybgpstream\nfrom collections import defaultdict\nfrom datetime import datetime, timedelta\nimport json\n\nstart_time = datetime.strptime(\"2015-08-10 07:00:00\", \"%Y-%m-%d %H:%M:%S\")\nend_time = datetime.strptime(\"2015-08-10 08:00:00\", \"%Y-%m-%d %H:%M:%S\")\n\ndata_structure = defaultdict(lambda: defaultdict(int))\n\nwhile start_time < end_time:\n    from_time_str = start_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    until_time_str = (start_time + timedelta(hours=1)).strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    stream = pybgpstream.BGPStream(\n        from_time=from_time_str, until_time=until_time_str,\n        collectors=[\"rrc06\"],\n        record_type=\"updates\",\n        filter=\"peer 25152 and prefix more 185.84.166.0/23 and community *:3400\"\n    )\n\n    current_prefix_count = defaultdict(int)\n    for rec in stream.records():\n        for elem in rec:\n            pfx = elem.fields['prefix']\n            communities = elem.fields['communities']\n            for community in communities:\n                current_prefix_count[community] += 1\n\n    for community, count in current_prefix_count.items():\n        data_structure[community][from_time_str] = count\n\n    start_time += timedelta(hours=1)\n\noutput = []\nfor community, time_series in data_structure.items():\n    output.append({\"Community\": community, \"Prefix Counts\": time_series})\n\nfor entry in output:\n    print(json.dumps(entry))"}], "is_classification": false}
{"id": "seed_task_34", "name": "Real-time Anomaly Detection in BGP Routes", "instruction": "Write a Python script using the PyBGPStream library to detect real-time anomalies in BGP routes. Flag and alert when: A new AS path is observed for an existing prefix. A known prefix suddenly disappears from the stream.", "instances": [{"input": "Python script using PyBGPStream library for real-time BGP anomaly detection.", "output": "import pybgpstream\nfrom collections import defaultdict\n\n# Initialize the stream\nstream = pybgpstream.BGPStream(\n    project=\"routeviews-stream\",\n    filter=\"router amsix\",\n)\n\ncurrent_as_paths = defaultdict(set)\n\nfor elem in stream:\n    if elem.type == \"A\":\n        pfx = elem.fields[\"prefix\"]\n        as_path = elem.fields[\"as-path\"]\n        if as_path not in current_as_paths[pfx]:\n            print(f\"Anomaly detected! New AS path {as_path} for prefix {pfx}\")\n            current_as_paths[pfx].add(as_path)\n    elif elem.type == \"W\":\n        pfx = elem.fields[\"prefix\"]\n        if pfx in current_as_paths:\n            print(f\"Anomaly detected! Prefix {pfx} withdrawn!\")\n            del current_as_paths[pfx]"}], "is_classification": false}
{"id": "seed_task_35", "name": "Real-Time Detection of Short-lived Prefix Announcements (Prefix Flapping)", "instruction": "Write a Python script using the PyBGPStream library for Real-Time Detection of Short-lived Prefix Announcements (Prefix Flapping). Detect when a prefix is announced and withdrawn repeatedly in a short time window (prefix flapping), which may indicate network instability or misconfiguration.", "instances": [{"input": "Python script using PyBGPStream library for real-time detection of prefix flapping.", "output": "import pybgpstream\nfrom collections import defaultdict\nimport time\n\nstream = pybgpstream.BGPStream(project=\"routeviews-stream\")\nflap_count = defaultdict(int)\ntimestamp_cache = defaultdict(int)\n\nfor elem in stream:\n    prefix = elem.fields.get('prefix', None)\n    if prefix:\n        current_time = int(time.time())\n        last_time = timestamp_cache.get(prefix, 0)\n        if current_time - last_time <= 60:\n            flap_count[prefix] += 1\n            if flap_count[prefix] > 5:\n                print(f\"Anomaly Detected! Prefix {prefix} is flapping.\")\n        else:\n            flap_count[prefix] = 0\n        timestamp_cache[prefix] = current_time"}], "is_classification": false}
{"id": "seed_task_36", "name": "Real-Time Detection of Subnet Changes", "instruction": "Write a Python script using the PyBGPStream library for Real-Time Detection of Subnet Changes. Detect when a larger prefix is suddenly divided into smaller subnets or vice versa, which may indicate potential re-allocation of IP space.", "instances": [{"input": "Python script using PyBGPStream library for real-time detection of subnet changes.", "output": "import pybgpstream\nfrom netaddr import IPNetwork\nfrom collections import defaultdict\n\nstream = pybgpstream.BGPStream(project=\"routeviews-stream\")\nknown_prefixes = defaultdict(set)\n\nfor elem in stream:\n    new_prefix = IPNetwork(elem.fields.get('prefix', '0.0.0.0/0'))\n    for known_prefix in known_prefixes:\n        if new_prefix in known_prefix or known_prefix in new_prefix:\n            print(f\"Anomaly Detected! Change in subnet: {known_prefix} -> {new_prefix}\")\n    known_prefixes[new_prefix] = True"}], "is_classification": false}
{"id": "seed_task_37", "name": "Real-Time Detection of BGP Session Resets", "instruction": "Write a Python script using the PyBGPStream library for Real-Time Detection of BGP Session Resets. Monitor for the mass withdrawal of routes from a single AS, which may indicate that the AS has reset its BGP session, possibly due to an issue.", "instances": [{"input": "Python script using PyBGPStream library for real-time detection of BGP session resets.", "output": "import pybgpstream\nfrom collections import defaultdict\n\nstream = pybgpstream.BGPStream(project=\"routeviews-stream\")\nwithdraw_count = defaultdict(int)\n\nfor elem in stream:\n    if elem.type == \"W\":\n        as_path = elem.fields.get('as-path', 'unknown')\n        withdraw_count[as_path] += 1\n\n        if withdraw_count[as_path] > 100:\n            print(f\"Anomaly Detected! Mass withdrawal from AS: {as_path}\")\n\n            # Reset count to avoid repetitive alerts\n            withdraw_count[as_path] = 0"}], "is_classification": false}
{"id": "seed_task_38", "name": "Investigation of Historical Trends in BGP Path Length", "instruction": "Investigate the historical trends in BGP path length using data collected from the rrc02 collector between 2023-01-01 and 2023-08-01. Perform a time-series analysis to calculate the average BGP path length for each month during this period. Visualize the results using a line chart and annotations indicating significant events that may have influenced path length changes.", "instances": [{"input": "Python script using PyBGPStream and Matplotlib for investigating historical trends in BGP path length.", "output": "import pybgpstream\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom collections import defaultdict\nfrom datetime import datetime\n\nstream = pybgpstream.BGPStream(\n    from_time=\"2023-01-01\", until_time=\"2023-08-01\",\n    collectors=\"rrc02\",\n    record_type=\"ribs\",\n)\n\nmonthly_path_lengths = defaultdict(list)\n\nfor rec in stream.records():\n    for elem in rec:\n        timestamp = datetime.fromtimestamp(rec.time)\n        path_length = len(elem.fields['as-path'].split())\n        month = timestamp.strftime(\"%Y-%m\")\n        monthly_path_lengths[month].append(path_length)\n\naverage_path_lengths = {month: sum(lengths) / len(lengths) for month, lengths in monthly_path_lengths.items()}\n\ndf = pd.DataFrame(list(average_path_lengths.items()), columns=['Month', 'Average Path Length'])\n\ndf['Month'] = pd.to_datetime(df['Month'])\n\ndf = df.sort_values('Month')\n\nplt.figure(figsize=(10, 6))\nplt.plot(df['Month'], df['Average Path Length'], marker='o')\nplt.xlabel(\"Month\")\nplt.ylabel(\"Average Path Length\")\nplt.title(\"Historical Trends in BGP Path Length\")\n\nplt.annotate(\"Event A\", xy=(pd.Timestamp('2023-03-01'), 6), xytext=(pd.Timestamp('2023-02-01'), 6.5),\n             arrowprops=dict(facecolor='black', arrowstyle=\"->\"),\n             )\n\nplt.annotate(\"Event B\", xy=(pd.Timestamp('2023-06-01'), 5), xytext=(pd.Timestamp('2023-06-20'), 5.5),\n             arrowprops=dict(facecolor='black', arrowstyle=\"->\"),\n             )\n\nplt.tight_layout()\nplt.show()"}], "is_classification": false}
{"id": "seed_task_39", "name": "Exploration of AS Relationship Influence on BGP Path Inflation", "instruction": "Explore the influence of various types of AS relationships—Peer, Customer, and Provider—on the inflation of BGP paths. Utilize BGP data gathered from collectors rrc01 and rrc02 between 14:00 and 15:00 on September 15, 2023. Construct a map to define the relationship type between each AS pair. Compute the effective BGP path length and the shortest path length for each AS pair in the graph. Output a comprehensive list of AS pairs, their associated relationship type, and indicate whether the BGP path is elongated compared to the graph's shortest path.", "instances": [{"input": "Python script using PyBGPStream and NetworkX for exploring AS relationship influence on BGP path inflation.", "output": "import pybgpstream\nimport networkx as nx\nfrom collections import defaultdict\nfrom datetime import datetime\n\nas_relationships = defaultdict(lambda: defaultdict(lambda: None))\nbgp_path_lengths = defaultdict(lambda: defaultdict(lambda: None))\nshortest_path_lengths = defaultdict(lambda: defaultdict(lambda: None))\n\nstream = pybgpstream.BGPStream(\n    from_time=\"2023-09-15 14:00:00\", until_time=\"2023-09-15 15:00:00\",\n    collectors=\"rrc01,rrc02\",\n    record_type=\"ribs\",\n)\n\nas_graph = nx.DiGraph()\n\nfor rec in stream.records():\n    for elem in rec:\n        peer = str(elem.peer_asn)\n        origin = elem.fields['as-path'].split()[-1]\n        as_graph.add_edge(peer, origin)\n        if len(elem.fields['as-path'].split()) <= 2:\n            as_relationships[peer][origin] = \"Peer\"\n        elif peer == elem.fields['as-path'].split()[0]:\n            as_relationships[peer][origin] = \"Provider\"\n        else:\n            as_relationships[peer][origin] = \"Customer\"\n        bgp_path_lengths[peer][origin] = len(elem.fields['as-path'].split())\n        shortest_path_lengths[peer][origin] = nx.shortest_path_length(as_graph, peer, origin)\n\nfor peer in bgp_path_lengths:\n    for origin in bgp_path_lengths[peer]:\n        rel_type = as_relationships[peer][origin]\n        bgp_length = bgp_path_lengths[peer][origin]\n        shortest_length = shortest_path_lengths[peer][origin]\n        path_difference = bgp_length - shortest_length\n        print(f\"AS Pair: {peer} -> {origin}\")\n        print(f\"  Relationship: {rel_type}\")\n        print(f\"  BGP Path Length: {bgp_length}\")\n        print(f\"  Shortest Path Length: {shortest_length}\")\n        print(f\"  Path Inflation: {'Inflated' if path_difference > 0 else 'Not Inflated'}\")"}], "is_classification": false}
{"id": "seed_task_40", "name": "Investigation of Historical Trends in BGP Path Length with Network Events", "instruction": "Investigate the historical trends in BGP path length using data collected from the rrc02 collector between 2023-01-01 and 2023-08-01. Perform a time-series analysis to calculate the average BGP path length for each month during this period. Visualize the results using a line chart and annotations indicating significant events that may have influenced path length changes.", "instances": [{"input": "Python script using PyBGPStream to investigate historical trends in BGP path length with network events.", "output": "import pybgpstream\n\nnetwork_events = {\n    \"Event A\": \"2023-03-10 12:30:00\",\n    \"Event B\": \"2023-03-10 13:15:00\",\n    # Add more events and timestamps here\n}\n\nstream = pybgpstream.BGPStream(\n    from_time=\"2023-03-10 12:00:00\", until_time=\"2023-03-10 14:00:00\",\n    collectors=\"rrc04\",\n    record_type=\"ribs\",\n)\n\nfor rec in stream.records():\n    timestamp = rec.time\n    for elem in rec:\n        prefix = elem.fields['prefix']\n        as_path = elem.fields['as-path'].split()\n        for event, event_time in network_events.items():\n            if abs((timestamp - pybgpstream.time.Time().from_str(event_time)).total_seconds()) <= 900:\n                print(f\"Event: {event}\")\n                print(f\"  Timestamp: {timestamp}\")\n                print(f\"  Prefix: {prefix}\")\n                print(f\"  AS Path: {as_path}\")\n                print(\"\\n\")"}], "is_classification": false}
{"id": "seed_task_41", "name": "Analysis of AS Path Preferences and AS Topology", "instruction": "Analyze the preferences of AS paths using data gathered from the collector rrc07 between 08:00 and 09:00 on May 20, 2023. Examine the most common AS paths and the frequency of their occurrence. Visualize the AS topology using a network graph. Output the list of top AS paths and their corresponding frequencies.", "instances": [{"input": "Python script using PyBGPStream, NetworkX, and Matplotlib to analyze AS path preferences and visualize AS topology.", "output": "import pybgpstream\nimport networkx as nx\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\n\nstream = pybgpstream.BGPStream(\n    from_time=\"2023-05-20 08:00:00\", until_time=\"2023-05-20 09:00:00\",\n    collectors=\"rrc07\",\n    record_type=\"ribs\",\n)\n\nas_path_frequencies = defaultdict(int)\n\nfor rec in stream.records():\n    for elem in rec:\n        as_path = tuple(elem.fields['as-path'].split())\n        as_path_frequencies[as_path] += 1\n\n# Display top AS paths and their frequencies\ntop_paths = sorted(as_path_frequencies.items(), key=lambda x: x[1], reverse=True)[:10]\nfor path, frequency in top_paths:\n    print(f\"AS Path: {' -> '.join(path)}, Frequency: {frequency}\")\n\nas_graph = nx.Graph()\nfor as_path in as_path_frequencies:\n    for i in range(len(as_path) - 1):\n        as_graph.add_edge(as_path[i], as_path[i + 1])\n\nplt.figure(figsize=(10, 6))\npos = nx.spring_layout(as_graph, seed=42)\nnx.draw(as_graph, pos, with_labels=True, font_size=8, node_size=100, node_color='skyblue')\nplt.title(\"AS Topology\")\nplt.show()"}], "is_classification": false}
{"id": "seed_task_42", "name": "Real-Time Anomaly Detection in BGP Routes", "instruction": "Write a Python script using the PyBGPStream library to detect real-time anomalies in BGP (Border Gateway Protocol) routes. Flag and alert when: A new AS path is observed for an existing prefix. A known prefix suddenly disappears from the stream.", "instances": [{"input": "Python script using PyBGPStream for real-time anomaly detection in BGP routes.", "output": "import pybgpstream\nfrom collections import defaultdict\n\nstream = pybgpstream.BGPStream(\n    project=\"routeviews-stream\",\n    filter=\"router amsix\",\n)\n\ncurrent_as_paths = defaultdict(set)\n\nfor elem in stream:\n    if elem.type == \"A\":\n        pfx = elem.fields[\"prefix\"]\n        as_path = elem.fields[\"as-path\"]\n        if as_path not in current_as_paths[pfx]:\n            print(f\"Anomaly detected! New AS path {as_path} for prefix {pfx}\")\n            current_as_paths[pfx].add(as_path)\n    elif elem.type == \"W\":\n        pfx = elem.fields[\"prefix\"]\n        if pfx in current_as_paths:\n            print(f\"Anomaly detected! Prefix {pfx} withdrawn!\")\n            del current_as_paths[pfx]"}], "is_classification": false}
{"id": "seed_task_43", "name": "Real-Time Detection of Short-lived Prefix Announcements (Prefix Flapping)", "instruction": "Write a Python script using the PyBGPStream library for Real-Time Detection of Short-lived Prefix Announcements (Prefix Flapping). Detect when a prefix is announced and withdrawn repeatedly in a short time window (prefix flapping), which may indicate network instability or misconfiguration.", "instances": [{"input": "Python script using PyBGPStream for real-time detection of prefix flapping.", "output": "import pybgpstream\nfrom collections import defaultdict\nimport time\n\nstream = pybgpstream.BGPStream(project=\"routeviews-stream\")\nflap_count = defaultdict(int)\ntimestamp_cache = defaultdict(int)\n\nfor elem in stream:\n    prefix = elem.fields.get('prefix', None)\n    if prefix:\n        current_time = int(time.time())\n        last_time = timestamp_cache.get(prefix, 0)\n        if current_time - last_time <= 60:\n            flap_count[prefix] += 1\n            if flap_count[prefix] > 5:\n                print(f\"Anomaly Detected! Prefix {prefix} is flapping.\")\n        else:\n            flap_count[prefix] = 0\n        timestamp_cache[prefix] = current_time"}], "is_classification": false}
{"id": "seed_task_44", "name": "Real-Time Detection of Subnet Changes with Advanced Analysis", "instruction": "Write a Python script using the PyBGPStream library for Real-Time Detection of Subnet Changes. Detect when a larger prefix is suddenly divided into smaller subnets or vice versa, which may indicate potential re-allocation of IP space.", "instances": [{"input": "Python script using PyBGPStream for real-time detection of subnet changes with advanced analysis.", "output": "import pybgpstream\nimport networkx as nx\nfrom collections import defaultdict\nfrom datetime import datetime\n\nas_graph = nx.Graph()\nbgp_lens = defaultdict(lambda: defaultdict(lambda: None))\ntime_based_bgp_lens = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: None)))\n\nstream = pybgpstream.BGPStream(\n    from_time=\"2022-01-01 00:00:00\", until_time=\"2022-06-30 23:59:59\",\n    project=\"route-views\",\n    record_type=\"updates\",\n    filter=\"ipversion 4\"\n)\n\nfor rec in stream.records():\n    timestamp = datetime.utcfromtimestamp(rec.time)\n    for elem in rec:\n        peer = str(elem.peer_asn)\n        hops = elem.fields['as-path'].split(\" \")\n        if len(hops) > 1 and str(hops[0]) == peer:\n            origin = str(hops[-1])\n            for i in range(0, len(hops) - 1):\n                as_graph.add_edge(str(hops[i]), str(hops[i + 1]))\n            bgp_lens[peer][origin] = min(list(filter(bool, [bgp_lens[peer][origin], len(hops)])))\n            time_based_bgp_lens[timestamp][peer][origin] = \\\n                min(list(filter(bool, [time_based_bgp_lens[timestamp][peer][origin], len(hops)])))\n\nfor timestamp in sorted(time_based_bgp_lens.keys()):\n    total_inflation = 0\n    count = 0\n    for peer in time_based_bgp_lens[timestamp]:\n        for origin in time_based_bgp_lens[timestamp][peer]:\n            try:\n                nxlen = len(nx.shortest_path(as_graph, peer, origin)) - 1\n                inflation = time_based_bgp_lens[timestamp][peer][origin] - nxlen\n                total_inflation += inflation\n                count += 1\n            except nx.NetworkXNoPath:\n                continue\n    average_inflation = total_inflation / count if count > 0 else 0\n    print(f\"Timestamp: {timestamp}, Average Path Inflation: {average_inflation}\")"}], "is_classification": false}
{"id": "seed_task_45", "name": "Analyzing BGP Updates for Peer 65001 and Prefix 192.0.2.0/24", "instruction": "Collect BGP data from the 'rrc06' collector within the time window of January 1, 2021, 00:00:00 to January 1, 2021, 01:00:00. Focus on the 'updates' record type with peer ASN 65001 and prefix 192.0.2.0/24.", "instances": [{"input": "BGP data collected from the 'rrc06' collector within the specified time window with the given filter.", "output": "Here is the collected BGP data:\n\nimport pybgpstream\n\nstream = pybgpstream.BGPStream(\n    from_time=\"2021-01-01 00:00:00\",\n    until_time=\"2021-01-01 01:00:00\",\n    project=\"RIS\",\n    collectors=[\"rrc06\"],\n    record_type=\"updates\",\n    filter=\"type updates and collector rrc06 and peer 65001 and prefix 192.0.2.0/24\"\n)\n\nfor elem in stream:\n    print(elem)"}], "is_classification": false}
{"id": "seed_task_46", "name": "Analyzing BGP Update Patterns for Peer 32934", "instruction": "Analyze BGP update events related to peer 32934. Focus on announcements (A) and withdrawals (W) on October 10, 2021, from 3pm to 10pm. Count these events and identify patterns like quick withdrawals and spikes in event numbers.", "instances": [{"input": "BGP data collected from October 10, 2021, 3pm to 10pm, related to peer 32934.", "output": "from datetime import datetime\nimport math\n\nstream = pybgpstream.BGPStream(\n    from_time=\"2021-10-04 15:00:00\", until_time=\"2021-10-04 22:00:00\",\n    record_type=\"updates\",\n    filter='peer 32934'\n)\nw_events = []\na_events = []\nOFFSET = 1633360000\n\n# ... (Rest of the output code) ..."}], "is_classification": false}
{"id": "seed_task_47", "name": "Retrieving IPv6 BGP Announcement Records", "instruction": "Retrieve BGP announcement records for IPv6 prefixes from the collector 'route-views.eqix'.", "instances": [{"input": "Retrieving BGP announcement records for IPv6 prefixes from the collector 'route-views.eqix'.", "output": "import pybgpstream\n\nstream = pybgpstream.BGPStream(\n    from_time=\"2022-01-01 00:00:00\", until_time=\"2022-01-01 01:00:00 UTC\",\n    collectors=[\"route-views.eqix\"],\n    filter=\"ipversion 6 and elemtype announcements\",\n    record_type=\"updates\"\n)\n\nfor elem in stream:\n    print(elem)"}], "is_classification": false}
{"id": "seed_task_48", "name": "Retrieving BGP Peer State Change Records", "instruction": "Retrieve BGP peer state change records for peer ASN 65500.", "instances": [{"input": "Retrieving BGP peer state change records for peer ASN 65500.", "output": "import pybgpstream\n\nstream = pybgpstream.BGPStream(\n    from_time=\"2022-01-01 00:00:00\", until_time=\"2022-01-01 01:00:00 UTC\",\n    filter=\"peer 65500 and elemtype peerstates\",\n    record_type=\"updates\"\n)\n\nfor elem in stream:\n    print(elem)"}], "is_classification": false}
{"id": "seed_task_49", "name": "Collecting BGP RIB Entries for IPv6 Prefixes with ASN 56789", "instruction": "Collect BGP RIB entries from a specific collector for IPv6 prefixes announced by AS path containing ASN 56789.", "instances": [{"input": "Collecting BGP RIB entries for IPv6 prefixes announced by AS path containing ASN 56789 from a specific collector.", "output": "import pybgpstream\n\nstream = pybgpstream.BGPStream(\n    from_time=\"2023-02-15 12:00:00\",\n    until_time=\"2023-02-15 13:00:00\",\n    collectors=[\"route-views.sg\"],\n    record_type=\"ribs\",\n    filter=\"type ribs and collector route-views.sg and ipversion 6 and path '_56789_'\",\n)\n\nfor elem in stream:\n    print(elem)"}], "is_classification": false}
{"id": "seed_task_50", "name": "Collecting BGP Updates for IPv4 Prefixes with Specific BGP Community", "instruction": "Collect BGP updates from a specific collector for IPv4 prefixes with a specific BGP community value.", "instances": [{"input": "Collecting BGP updates for IPv4 prefixes with a specific BGP community value from a specific collector.", "output": "import pybgpstream\n\nstream = pybgpstream.BGPStream(\n    from_time=\"2023-03-10 15:00:00\",\n    until_time=\"2023-03-10 16:00:00\",\n    collectors=[\"rrc10\"],\n    record_type=\"updates\",\n    filter=\"type updates and collector rrc10 and prefix more 0.0.0.0/0 and comm 12345:6789\"\n)\n\nfor elem in stream:\n    print(elem)"}], "is_classification": false}
{"id": "seed_task_51", "name": "Analyzing BGP Update Events for Prefix '203.0.113.0/24'", "instruction": "Examine the Border Gateway Protocol (BGP) update events for the prefix '203.0.113.0/24' during September 1, 2023, from 9 AM to 6 PM. Calculate the percentage of time the prefix was withdrawn and the average interval between announcements.", "instances": [{"input": "Analyzing BGP update events for the prefix '203.0.113.0/24' during September 1, 2023, from 9 AM to 6 PM.", "output": "from datetime import datetime, timedelta\nimport pybgpstream\n\nfrom_time_str = \"2023-09-01 09:00:00\"\nuntil_time_str = \"2023-09-01 18:00:00\"\nfrom_time = datetime.strptime(from_time_str, '%Y-%m-%d %H:%M:%S')\nuntil_time = datetime.strptime(until_time_str, '%Y-%m-%d %H:%M:%S')\n\ntotal_interval = (until_time - from_time).total_seconds()\n\nstream = pybgpstream.BGPStream(\n    from_time=from_time_str, until_time=until_time_str,\n    record_type=\"updates\",\n    filter='prefix exact 203.0.113.0/24'\n)\n\nwithdrawn_time = 0\nannouncement_intervals = []\nlast_announcement_time = None\n\nfor elem in stream:\n    if elem.type == 'W':\n        withdrawn_time += 1\n    elif elem.type == 'A':\n        if last_announcement_time:\n            interval = elem.time - last_announcement_time\n            announcement_intervals.append(interval)\n        last_announcement_time = elem.time\n\nif total_interval:\n    withdrawn_percentage = (withdrawn_time / total_interval) * 100\n    print(f\"Percentage of time prefix withdrawn: {withdrawn_percentage:.2f}%\")\n\nif announcement_intervals:\n    avg_interval = sum(announcement_intervals) / len(announcement_intervals)\n    print(f\"Average interval between announcements: {avg_interval:.2f} seconds\")"}], "is_classification": false}
{"id": "seed_task_52", "name": "Comparing BGP Update Events for Peers '65000' and '65100'", "instruction": "Compare BGP update events from peer '65000' and peer '65100' during November 15, 2022, from 2 PM to 8 PM. Determine if there are significant differences in the frequency of announcements and withdrawals between the two peers.", "instances": [{"input": "Comparing BGP update events from peers '65000' and '65100' during November 15, 2022, from 2 PM to 8 PM.", "output": "from datetime import datetime\nimport pybgpstream\n\npeers = ['65000', '65100']\n\nstream = pybgpstream.BGPStream(\n    from_time=\"2022-11-15 14:00:00\", until_time=\"2022-11-15 20:00:00\",\n    record_type=\"updates\"\n)\n\npeer_data = {peer: {'announcements': 0, 'withdrawals': 0} for peer in peers}\n\nfor elem in stream:\n    peer_asn = str(elem.peer_asn)\n    if peer_asn not in peer_data:\n        continue\n    if elem.type == 'A':\n        peer_data[peer_asn]['announcements'] += 1\n    elif elem.type == 'W':\n        peer_data[peer_asn]['withdrawals'] += 1\n\nfor peer, data in peer_data.items():\n    print(f\"Peer {peer}:\")\n    print(f\"Number of announcements: {data['announcements']}\")\n    print(f\"Number of withdrawals: {data['withdrawals']}\")"}], "is_classification": false}
{"id": "seed_task_53", "name": "Analyzing BGP Update Events for AS Path Patterns '64512' and '65500'", "instruction": "Analyze BGP update events with AS path containing '64512' and '65500' during July 20, 2022, from 10 AM to 4 PM. Count the occurrences of different AS path patterns and identify the most common ones.", "instances": [{"input": "Analyzing BGP update events with AS path containing '64512' and '65500' during July 20, 2022, from 10 AM to 4 PM.", "output": "import pybgpstream\n\nstream = pybgpstream.BGPStream(\n    from_time=\"2022-07-20 10:00:00\", until_time=\"2022-07-20 16:00:00\",\n    record_type=\"updates\",\n    filter='path \"64512\" and path \"65500\"'\n)\n\nas_path_patterns = {}\n\nfor elem in stream:\n    as_path = elem.fields['as-path']\n    if as_path not in as_path_patterns:\n        as_path_patterns[as_path] = 1\n    else:\n        as_path_patterns[as_path] += 1\n\nsorted_patterns = sorted(as_path_patterns.items(), key=lambda x: x[1], reverse=True)\n\nprint(\"Most common AS path patterns:\")\nfor pattern, count in sorted_patterns[:5]:\n    print(f\"Pattern: {pattern}, Count: {count}\")"}], "is_classification": false}